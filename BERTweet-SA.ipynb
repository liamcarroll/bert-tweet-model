{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERTweet-SA.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMfmqS/4F6H631TsgONXbQb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qgbskhZI4Ro9"},"source":["# Sentiment Analysis of Tweets with BERT"]},{"cell_type":"markdown","metadata":{"id":"rjW698Sp4V4C"},"source":["One BERT model which caught my eye when scrolling through the Huggingface website was the BERTweet model. The BERTweet model was proposed in the paper below:\n","\n","[BERTweet: A pre-trained language model for English Tweets](https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf)\n","\n","BERTweet is the first public large-scale pre-trained language model for English Tweets. The corpus used to pre-train BERTweet consists of 850 million English Tweets.\n","\n","In this notebook, I am going to attempt to fine-tune the BERTweet model for the purpose of sentiment analysis of Tweets."]},{"cell_type":"markdown","metadata":{"id":"NBjXTL5Y4dSF"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"wckIWNgG4jXP"},"source":["### Mounting Google Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uWLOwkrz4QsU","executionInfo":{"status":"ok","timestamp":1619216376631,"user_tz":-60,"elapsed":585,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}},"outputId":"2f65654d-5d38-48ee-8e6a-5ca4407912f4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oLwt-x7j5tuF"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"JDldBpnS4mUM","executionInfo":{"status":"ok","timestamp":1619216380976,"user_tz":-60,"elapsed":535,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["import sys\n","import os\n","os.chdir('/content/drive/My Drive/College - 4th Year/CA4023_NLT/Assignment3')\n","\n","try:\n","    import torch\n","except ModuleNotFoundError:\n","    !{sys.executable} -m pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","try:\n","    import transformers\n","except ModuleNotFoundError:\n","    !{sys.executable} -m pip install transformers\n","\n","try:\n","    import emoji\n","except ModuleNotFoundError:\n","    !{sys.executable} -m pip install emoji"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"irVceUCS6bmJ","executionInfo":{"status":"ok","timestamp":1619216382998,"user_tz":-60,"elapsed":760,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["import pandas as pd"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"suX7BkQp6z6l"},"source":["## Loading Data"]},{"cell_type":"markdown","metadata":{"id":"boQqtPDAACAp"},"source":["For the purpose of sentiment prediction of Tweets, I will need a dataset consisting of Tweets annotated with sentiment. The dataset which I have chosen is the [sentiment140 dataset](https://www.kaggle.com/kazanova/sentiment140), which is freely available on Kaggle. This dataset contains 1.6 million Tweets annotated with sentiment (0 = negative, 4 = positive). \n","\n","For the sake of practicality, I am only going to use a subset of the 1.6 million Tweets. Below, I extract 50,000 positive and 50,000 negative sentiment Tweets to use.\n","\n"]},{"cell_type":"code","metadata":{"id":"bG7ciTzr83Fq","executionInfo":{"status":"ok","timestamp":1619216385231,"user_tz":-60,"elapsed":551,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["data_path = \"./data/tweets/training.1600000.processed.noemoticon.csv\""],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"vqzxvfEq609D","executionInfo":{"status":"ok","timestamp":1619216390340,"user_tz":-60,"elapsed":5442,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["df_tweets = pd.read_csv(data_path, names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"], encoding='latin-1')"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"pQB5I8UP9XE5","executionInfo":{"status":"ok","timestamp":1619216390349,"user_tz":-60,"elapsed":5212,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}},"outputId":"1ee75bee-5a9a-40d2-9716-1feaf89454e1"},"source":["df_tweets.head()"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>id</th>\n","      <th>date</th>\n","      <th>flag</th>\n","      <th>user</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1467810369</td>\n","      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>_TheSpecialOne_</td>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1467810672</td>\n","      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>scotthamilton</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1467810917</td>\n","      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>mattycus</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1467811184</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>ElleCTF</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1467811193</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>Karoli</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   target  ...                                               text\n","0       0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n","1       0  ...  is upset that he can't update his Facebook by ...\n","2       0  ...  @Kenichan I dived many times for the ball. Man...\n","3       0  ...    my whole body feels itchy and like its on fire \n","4       0  ...  @nationwideclass no, it's not behaving at all....\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"1nY7uIXA_cdp"},"source":["For the purpose of sentiment analysis, I only require the *target* and *text* columns. Therefore, I can drop the remaining columns."]},{"cell_type":"code","metadata":{"id":"UnnnngUl91ED","executionInfo":{"status":"ok","timestamp":1619216390351,"user_tz":-60,"elapsed":4758,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["df_tweets = df_tweets.drop(columns=[\"id\", \"date\", \"flag\", \"user\"])"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"G9wnAi0Q_vJp","executionInfo":{"status":"ok","timestamp":1619216390352,"user_tz":-60,"elapsed":4332,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}},"outputId":"72db6c13-0ae4-4b64-9ac7-8bde9228e8ba"},"source":["df_tweets.head()"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   target                                               text\n","0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n","1       0  is upset that he can't update his Facebook by ...\n","2       0  @Kenichan I dived many times for the ball. Man...\n","3       0    my whole body feels itchy and like its on fire \n","4       0  @nationwideclass no, it's not behaving at all...."]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"RpUmL1a6BTd8","executionInfo":{"status":"ok","timestamp":1619216390701,"user_tz":-60,"elapsed":4500,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["df_pos = df_tweets[df_tweets[\"target\"] == 0]\n","df_neg = df_tweets[df_tweets[\"target\"] == 4]"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gkh8TMYBBc7S","executionInfo":{"status":"ok","timestamp":1619216390702,"user_tz":-60,"elapsed":4272,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["# We only want 50,000 of each\n","df_pos = df_pos[:50000]\n","df_neg = df_neg[:50000]"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"GTcNQibmBc4S","executionInfo":{"status":"ok","timestamp":1619216390703,"user_tz":-60,"elapsed":4132,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["# Concatenate the above to form our new df\n","df = pd.concat([df_pos, df_neg], ignore_index=True)"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4eCr-7z1FPtf"},"source":["Next, we put this data into arrays as this is the data structure which will be use from here onwards."]},{"cell_type":"code","metadata":{"id":"1x3asB8UCTrY","executionInfo":{"status":"ok","timestamp":1619216390704,"user_tz":-60,"elapsed":3448,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["texts = list(df['text'])\n","labels = list(df['target'])"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bmqi0nG5FzA9"},"source":["The final step is to split the data into train, validation and test sets which we can use for evaluation and tuning."]},{"cell_type":"code","metadata":{"id":"tWzV0CXaF9jU","executionInfo":{"status":"ok","timestamp":1619216390705,"user_tz":-60,"elapsed":1992,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.1)"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cpmEjvYkGvCB"},"source":["A validation set is also useful to have..."]},{"cell_type":"code","metadata":{"id":"6LkuudBXGrPd","executionInfo":{"status":"ok","timestamp":1619216390706,"user_tz":-60,"elapsed":768,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2)"],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aE5MWgsiG9xI"},"source":["Below are the size of the resulting sets..."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49xdMfXqG72b","executionInfo":{"status":"ok","timestamp":1619216393216,"user_tz":-60,"elapsed":519,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}},"outputId":"56b28338-9dfa-4bd7-87a1-2009586c5cfa"},"source":["print(\"Training Set: \", len(train_texts))\n","print(\"Validation Set: \", len(val_texts))\n","print(\"Test Set: \", len(test_texts))"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Training Set:  72000\n","Validation Set:  18000\n","Test Set:  10000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LxxeUygb_zLr"},"source":["## Normalising Tweets"]},{"cell_type":"markdown","metadata":{"id":"Nfqg07UBHiCq"},"source":["Before fine-tuning the BERTweet model, the Tweets must also be normalized by converting user mentions and web/url links into special tokens @USER and HTTPURL, respectively. Thankfully, BERTweet provides this pre-processing step by enabling the normalization argument of the AutoTokenizer."]},{"cell_type":"code","metadata":{"id":"FL5XVPI5_2in","executionInfo":{"status":"ok","timestamp":1619217064890,"user_tz":-60,"elapsed":622,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["import torch\n","from transformers import AutoTokenizer"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bgRP5248IFP2"},"source":["As the input Tweets are raw, we load the BERTweet tokeniser with normalisation mode enabled."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGzVipHLAJuX","executionInfo":{"status":"ok","timestamp":1619216402513,"user_tz":-60,"elapsed":2176,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}},"outputId":"1c1a47ff-c487-468f-d800-01199777d78f"},"source":["tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"48Ak7hTtIWtz"},"source":["Now, we can simply pass the texts arrays to the tokeniser. The truncation and padding arguments ensure that Tweets are truncated to be no longer than the model's maximum input length and that all of the sequences are padded to the same length."]},{"cell_type":"code","metadata":{"id":"L1Uw3abrBLvU","executionInfo":{"status":"ok","timestamp":1619211524481,"user_tz":-60,"elapsed":77971,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UPBT3q9TWBup"},"source":["## Creating PyTorch Datasets"]},{"cell_type":"markdown","metadata":{"id":"MnPHsKAtWKYr"},"source":["The next step is to take the encodings and labels which we have and turn these into a PyTorch *Dataset* object. This can be done by simply subclassing a *torch.utils.data.Dataset* object and implementing some functions. The data is put into this format so that it can be easily batched during training."]},{"cell_type":"code","metadata":{"id":"yl03XvetU1Um","executionInfo":{"status":"ok","timestamp":1619211524482,"user_tz":-60,"elapsed":77963,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["class TweetsDataset(torch.utils.data.Dataset):\n","  def __init__(self, encodings, labels):\n","    self.encodings = encodings\n","    self.labels = labels\n","\n","  def __getitem__(self, idx):\n","    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","    item['labels'] = torch.tensor(self.labels[idx])\n","    return item\n","\n","  def __len__(self):\n","    return len(self.labels)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c-AOkeg0XXSX"},"source":["Now, we can create our train, val and test datasets."]},{"cell_type":"code","metadata":{"id":"na_nV5jcXVyd","executionInfo":{"status":"ok","timestamp":1619211524483,"user_tz":-60,"elapsed":77953,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["train_dataset = TweetsDataset(train_encodings, train_labels)\n","val_dataset = TweetsDataset(val_encodings, val_labels)\n","test_dataset = TweetsDataset(test_encodings, test_labels)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RJB96JGqX61I"},"source":["## Fine-tuning BERTweet"]},{"cell_type":"markdown","metadata":{"id":"N4TiG0LdX9pJ"},"source":["Now that the datasets are ready, the final step is to fine-tune the BERTweet model. The *transformers* package provides a very useful Trainer object which can be used for this task.\n","\n","We simply need to define our model, define the TrainingArguments and instantiate a Trainer."]},{"cell_type":"code","metadata":{"id":"1O6B20lsXpnZ","executionInfo":{"status":"ok","timestamp":1619214439456,"user_tz":-60,"elapsed":466,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}}},"source":["from transformers import AutoModel, Trainer, TrainingArguments"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":346},"id":"2ui1WR1PdxXz","executionInfo":{"status":"error","timestamp":1619217104844,"user_tz":-60,"elapsed":7843,"user":{"displayName":"Liam Carroll","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPtQndoXywClHl3VxVV1xBx7gSdj-klDov-KEw=s64","userId":"06888516636468107256"}},"outputId":"0a4d2501-3e50-46e2-bff8-0e7a41aebb4b"},"source":["training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=3,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=10,\n",")\n","\n","model = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")\n","\n","trainer.train()"],"execution_count":49,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-77da50d7ce8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1524\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1556\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1557\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'labels'"]}]},{"cell_type":"markdown","metadata":{"id":"LfPi5U-O3DNM"},"source":["Unfortunately, the model did not train sucessfully due to an error and after much trial and error, I could not figure out the issue."]},{"cell_type":"code","metadata":{"id":"lWwqnX3Pk_vk"},"source":[""],"execution_count":null,"outputs":[]}]}